# scraper_app.py
# Run with: streamlit run scraper_app.py
# Requires: pip install streamlit requests beautifulsoup4 lxml markdownify

import streamlit as st
import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md
from urllib.parse import urljoin, urlparse, urldefrag
from urllib.robotparser import RobotFileParser
import time
import hashlib
import re
from collections import OrderedDict

# ============ CONFIGURATION ============
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
MIN_CONTENT_LENGTH = 150
MIN_PARAGRAPH_WORDS = 8

# Layout detection patterns
LAYOUT_PATTERNS = {
    "hero": {
        "classes": ["hero", "banner", "jumbotron", "masthead", "splash", "cover", "intro-section"],
        "ids": ["hero", "banner", "masthead", "intro"],
    },
    "testimonial": {
        "classes": ["testimonial", "quote", "review", "customer-quote", "client-quote", "blockquote"],
        "tags": ["blockquote"],
    },
    "faq": {
        "classes": ["faq", "accordion", "collapsible", "questions", "q-and-a"],
        "tags": ["details"],
    },
    "cta": {
        "classes": ["cta", "call-to-action", "action-box", "signup", "get-started"],
    },
    "gallery": {
        "classes": ["gallery", "image-grid", "photo-grid", "lightbox", "carousel", "slider"],
    },
    "features": {
        "classes": ["features", "benefits", "services", "offerings", "highlights"],
    },
    "pricing": {
        "classes": ["pricing", "plans", "packages", "tiers"],
    },
    "team": {
        "classes": ["team", "staff", "people", "about-us", "our-team"],
    },
    "contact": {
        "classes": ["contact", "get-in-touch", "reach-us"],
    },
}

JUNK_SELECTORS = [
    "nav", "header", "footer", ".navbar", ".menu", ".sidebar",
    ".footer", ".copyright", ".social-links", ".cookie", ".popup",
    "form", "input", "button", "select", "textarea",
    ".btn", ".button", '[role="navigation"]', ".skip-link",
    ".site-header", ".site-footer", ".main-navigation",
    ".breadcrumb", ".pagination", ".comments", ".related-posts",
    "[aria-hidden='true']", ".screen-reader-text", ".sr-only"
]

JUNK_TEXT_PATTERNS = [
    "privacy policy", "terms of service", "cookie policy",
    "accept cookies", "all rights reserved", "Â©",
    "follow us on", "share on facebook", "tweet this",
    "subscribe to", "sign up for", "enter your email"
]

SKIP_EXTENSIONS = {
    '.pdf', '.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp',
    '.mp3', '.mp4', '.avi', '.mov', '.zip', '.tar', '.gz',
    '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx'
}


class WebScraper:
    def __init__(self, base_url, max_pages=100, delay=0.3, respect_robots=True, exclude_paths=None, single_page_mode=False):
        self.base_url = base_url.rstrip("/")
        self.single_page_mode = single_page_mode
        
        # In single page mode, use the exact URL provided
        if single_page_mode:
            self.start_url = base_url
        else:
            self.start_url = self.base_url + "/"
        
        self.max_pages = 1 if single_page_mode else max_pages
        self.delay = delay
        self.respect_robots = respect_robots
        self.exclude_paths = exclude_paths or []
        
        self.visited = set()
        self.to_visit = {self.start_url}
        self.unique_content = OrderedDict()
        self.content_hashes = set()
        
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": USER_AGENT})
        
        self.robots_parser = None
        self.status_messages = []
    
    def _load_robots_txt(self):
        if not self.respect_robots:
            return
        self.robots_parser = RobotFileParser()
        robots_url = urljoin(self.base_url, "/robots.txt")
        try:
            self.robots_parser.set_url(robots_url)
            self.robots_parser.read()
            self.status_messages.append(f"âœ“ Loaded robots.txt")
        except Exception as e:
            self.status_messages.append(f"âš  Could not load robots.txt: {e}")
            self.robots_parser = None
    
    def _can_fetch(self, url):
        if not self.robots_parser:
            return True
        return self.robots_parser.can_fetch(USER_AGENT, url)
    
    def _normalize_url(self, url):
        url, _ = urldefrag(url)
        url = url.rstrip("/")
        return url
    
    def _is_valid_url(self, url):
        parsed = urlparse(url)
        if parsed.netloc != urlparse(self.base_url).netloc:
            return False
        if parsed.query:
            return False
        path_lower = parsed.path.lower()
        if any(path_lower.endswith(ext) for ext in SKIP_EXTENSIONS):
            return False
        # Check excluded paths
        for exclude in self.exclude_paths:
            if exclude and exclude.lower() in path_lower:
                return False
        return True
    
    def _extract_title(self, soup, url):
        if soup.title and soup.title.string:
            title = soup.title.string.strip()
            if "|" in title:
                title = title.split("|")[0].strip()
            if "-" in title and len(title.split("-")) > 1:
                title = title.rsplit("-", 1)[0].strip()
            if title:
                return title
        h1 = soup.find("h1")
        if h1:
            title = h1.get_text().strip()
            if title:
                return title
        path = urlparse(url).path.strip("/")
        if path:
            return path.replace("-", " ").replace("/", " â†’ ").title()
        return "Home"
    
    def _is_junk_text(self, text):
        text_lower = text.lower()
        return any(pattern in text_lower for pattern in JUNK_TEXT_PATTERNS)
    
    def _detect_section_type(self, element, is_first=False):
        """Detect the type of section based on classes, IDs, structure, and position"""
        classes = " ".join(element.get("class", [])).lower()
        elem_id = (element.get("id") or "").lower()
        tag_name = element.name
        text_content = element.get_text(strip=True)
        text_lower = text_content.lower()
        
        # Check against known patterns first (class/id based detection)
        for layout_type, patterns in LAYOUT_PATTERNS.items():
            if "classes" in patterns:
                for cls in patterns["classes"]:
                    if cls in classes:
                        return layout_type
            if "ids" in patterns:
                for id_pattern in patterns["ids"]:
                    if id_pattern in elem_id:
                        return layout_type
            if "tags" in patterns:
                if tag_name in patterns["tags"]:
                    return layout_type
        
        # === POSITION HEURISTICS ===
        
        # First section with heading/image is likely hero
        if is_first:
            has_h1 = element.find("h1") is not None
            has_large_image = element.find("img") is not None
            has_bg_style = "background" in (element.get("style") or "").lower()
            if has_h1 or has_large_image or has_bg_style:
                return "hero"
        
        # === CONTENT HEURISTICS ===
        
        # Testimonial: quotes, blockquotes, or attribution patterns
        has_blockquote = element.find("blockquote") is not None
        has_quotes = any(q in text_content for q in ['"', '"', '"', 'Â«', 'Â»', "''"])
        has_citation = element.find("cite") is not None
        # Attribution patterns: "Name, City" or "- Name" or "â€” Name, Title"
        attribution_pattern = re.search(r'[â€”â€“-]\s*[A-Z][a-z]+(\s+[A-Z]\.?)?(\s+[A-Z][a-z]+)?,?\s*([\w\s]+,\s*[A-Z]{2})?', text_content)
        
        if has_blockquote or has_citation or (has_quotes and attribution_pattern):
            return "testimonial"
        
        # FAQ: multiple questions
        questions = text_content.count("?")
        has_details = element.find("details") is not None
        if questions >= 3 or has_details:
            return "faq"
        
        # Gallery: multiple images (3+)
        images = element.find_all("img")
        if len(images) >= 3:
            return "gallery"
        
        # Pricing: dollar signs or price patterns
        price_pattern = re.search(r'\$\d+|\d+\.\d{2}|/month|/year|per month|per year', text_lower)
        if price_pattern and ("plan" in text_lower or "price" in text_lower or "tier" in text_lower):
            return "pricing"
        
        # Features/benefits: list with short items, often with icons
        list_items = element.find_all("li")
        if len(list_items) >= 3:
            avg_item_length = sum(len(li.get_text()) for li in list_items) / len(list_items)
            if avg_item_length < 100:  # Short list items suggest features
                return "features"
        
        # Contact: contact-related keywords
        contact_keywords = ["email", "phone", "address", "contact us", "get in touch", "reach out"]
        if any(kw in text_lower for kw in contact_keywords):
            if element.find("a", href=lambda h: h and ("mailto:" in h or "tel:" in h)):
                return "contact"
        
        # CTA: short section with action words
        word_count = len(text_content.split())
        cta_phrases = ["book now", "get started", "sign up", "contact us", "learn more", 
                       "buy now", "subscribe", "join", "start free", "try free", "call us"]
        has_cta_text = any(cta in text_lower for cta in cta_phrases)
        if word_count < 75 and has_cta_text:
            return "cta"
        
        return None
    
    def _detect_column_layout(self, element):
        """Detect if element has a multi-column layout"""
        classes = " ".join(element.get("class", [])).lower()
        style = (element.get("style") or "").lower()
        
        # Bootstrap/common grid patterns
        col_patterns = {
            "two_column": ["col-6", "col-md-6", "col-lg-6", "two-col", "half", "split"],
            "three_column": ["col-4", "col-md-4", "col-lg-4", "three-col", "thirds"],
            "four_column": ["col-3", "col-md-3", "col-lg-3", "four-col", "quarters"],
        }
        
        for layout, patterns in col_patterns.items():
            for pattern in patterns:
                if pattern in classes:
                    return layout
        
        # Check for flexbox/grid in style
        if "display: flex" in style or "display: grid" in style:
            # Count direct children to estimate columns
            children = [c for c in element.children if hasattr(c, 'name') and c.name]
            if len(children) == 2:
                return "two_column"
            elif len(children) == 3:
                return "three_column"
            elif len(children) >= 4:
                return "four_column"
        
        return "single_column"
    
    def _has_image(self, element):
        """Check if element contains an image"""
        return bool(element.find(["img", "picture", "figure", "svg"]))
    
    def _extract_section_data(self, element, url, is_first=False):
        """Extract structured data from a section"""
        section_type = self._detect_section_type(element, is_first=is_first) or "content"
        column_layout = self._detect_column_layout(element)
        has_image = self._has_image(element)
        
        # Determine overall layout type
        if section_type == "hero":
            layout = "hero"
        elif section_type in ["testimonial", "faq", "cta", "gallery", "features", "pricing", "team", "contact"]:
            layout = section_type
        elif column_layout != "single_column":
            if has_image:
                layout = f"{column_layout}_text_image"
            else:
                layout = f"{column_layout}_text"
        else:
            layout = "single_column"
        
        # Extract headings
        h1 = element.find("h1")
        h2 = element.find("h2")
        h3 = element.find("h3")
        
        heading = h1.get_text(strip=True) if h1 else None
        subheading = (h2 or h3).get_text(strip=True) if (h2 or h3) else None
        
        # Extract images
        images = []
        for img in element.find_all("img", src=True):
            src = img.get("src", "")
            if src and not src.startswith("data:"):
                if not src.startswith("http"):
                    src = urljoin(url, src)
                images.append({
                    "src": src,
                    "alt": img.get("alt", ""),
                })
        
        # Get content as markdown
        content_md = md(
            str(element),
            heading_style="ATX",
            bullets="-",
            strong_em_symbol="*",
        )
        content_md = re.sub(r'\n{3,}', '\n\n', content_md).strip()
        
        return {
            "layout": layout,
            "section_type": section_type,
            "heading": heading,
            "subheading": subheading,
            "has_image": has_image,
            "images": images[:5],  # Limit to 5 images
            "content": content_md,
        }
    
    def _extract_content(self, html, url):
        soup = BeautifulSoup(html, "lxml")
        
        # Remove junk elements first
        for selector in JUNK_SELECTORS:
            for element in soup.select(selector):
                element.decompose()
        
        for tag in soup(["script", "style", "noscript", "meta", "link", "iframe", "svg", "path"]):
            tag.decompose()
        
        title = self._extract_title(soup, url)
        
        # Find main content area
        main_content = (
            soup.find("main") or 
            soup.find("article") or 
            soup.find(class_=lambda x: x and ("content" in x.lower() or "body" in x.lower())) or
            soup.find(id=lambda x: x and ("content" in x.lower() or "main" in x.lower())) or
            soup.body
        )
        
        if not main_content:
            main_content = soup
        
        # Extract sections with layout info
        sections = []
        
        # Look for semantic sections
        section_tags = main_content.find_all(["section", "article", "div"], recursive=False)
        
        # If no clear sections, treat the whole thing as one section
        if not section_tags:
            section_tags = [main_content]
        
        for idx, section_elem in enumerate(section_tags):
            # Skip empty or tiny sections
            text_content = section_elem.get_text(strip=True)
            if len(text_content) < 50:
                continue
            
            # First meaningful section is likely hero
            is_first_section = (len(sections) == 0)
            section_data = self._extract_section_data(section_elem, url, is_first=is_first_section)
            
            # Skip junk sections
            if section_data["content"] and not self._is_junk_text(section_data["content"][:200]):
                sections.append(section_data)
        
        # Also create a combined markdown version
        content = md(
            str(main_content),
            heading_style="ATX",
            bullets="-",
            strong_em_symbol="*",
        )
        
        # Clean up the markdown
        content = re.sub(r'\n{3,}', '\n\n', content)
        lines = [line.rstrip() for line in content.split('\n')]
        content = '\n'.join(lines)
        
        # Filter out junk lines
        filtered_lines = []
        for line in content.split('\n'):
            line_lower = line.lower().strip()
            if any(pattern in line_lower for pattern in JUNK_TEXT_PATTERNS):
                continue
            if line.strip() and not line.startswith('#') and len(line.strip()) < 15:
                if not line.strip().startswith('-') and not re.match(r'^\d+\.', line.strip()):
                    continue
            filtered_lines.append(line)
        
        content = '\n'.join(filtered_lines).strip()
        
        if len(content) < MIN_CONTENT_LENGTH:
            content = soup.get_text(separator="\n", strip=True)
            lines = [line.strip() for line in content.splitlines() if line.strip()]
            content = "\n".join(lines)
        
        return title, content, sections
    
    def _content_hash(self, text):
        normalized = " ".join(text.lower().split())
        return hashlib.md5(normalized.encode()).hexdigest()
    
    def _extract_links(self, html, current_url):
        soup = BeautifulSoup(html, "lxml")
        links = set()
        
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if href.startswith(("javascript:", "mailto:", "tel:", "#")):
                continue
            full_url = urljoin(current_url, href)
            normalized = self._normalize_url(full_url)
            if self._is_valid_url(normalized) and normalized not in self.visited:
                links.add(normalized)
        
        return links
    
    def crawl(self, progress_bar, status_text, stats_container):
        """Main crawl loop with Streamlit progress updates"""
        self._load_robots_txt()
        
        if self.single_page_mode:
            status_text.text(f"ðŸŽ¯ Single page mode: {self.start_url}")
        
        pages_processed = 0
        
        while self.to_visit and len(self.visited) < self.max_pages:
            url = self.to_visit.pop()
            
            if url in self.visited:
                continue
            
            if self.respect_robots and not self._can_fetch(url):
                self.status_messages.append(f"â›” Blocked by robots.txt: {url}")
                continue
            
            self.visited.add(url)
            pages_processed += 1
            
            # Update progress
            progress = min(pages_processed / self.max_pages, 1.0)
            progress_bar.progress(progress)
            status_text.text(f"Crawling: {url[:60]}...")
            
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                
                content_type = response.headers.get("Content-Type", "")
                if "text/html" not in content_type:
                    continue
                
            except requests.RequestException as e:
                self.status_messages.append(f"âŒ Failed: {url} ({str(e)[:50]})")
                continue
            
            title, content, sections = self._extract_content(response.text, url)
            
            content_hash = self._content_hash(content)
            if content_hash in self.content_hashes:
                continue
            
            if len(content) >= MIN_CONTENT_LENGTH:
                self.content_hashes.add(content_hash)
                self.unique_content[url] = {
                    "title": title,
                    "content": content,
                    "sections": sections,
                }
            
            new_links = self._extract_links(response.text, url)
            if not self.single_page_mode:
                self.to_visit.update(new_links)
            
            # Update stats
            stats_container.markdown(f"""
            **Pages processed:** {pages_processed}  
            **Unique pages saved:** {len(self.unique_content)}  
            **URLs in queue:** {len(self.to_visit)}
            """)
            
            time.sleep(self.delay)
        
        progress_bar.progress(1.0)
        status_text.text("âœ… Crawl complete!")
        
        return self.unique_content
    
    def to_markdown(self):
        """Export results as markdown string with layout annotations"""
        lines = [
            f"# {urlparse(self.base_url).netloc} â€“ Scraped Content\n",
            f"*Extracted {time.strftime('%Y-%m-%d %H:%M')}*",
            f"*{len(self.unique_content)} pages with unique content*\n",
            "---\n"
        ]
        
        for url, data in self.unique_content.items():
            lines.append(f"## {data['title']}\n")
            lines.append(f"**URL:** {url}\n")
            
            # Add layout summary
            if data.get("sections"):
                layouts = [s["layout"] for s in data["sections"] if s.get("layout")]
                if layouts:
                    lines.append(f"**Layouts:** {', '.join(layouts)}\n")
            
            lines.append(f"\n{data['content']}\n")
            lines.append("---\n")
        
        return "\n".join(lines)
    
    def to_json(self):
        """Export results as structured JSON with layout data"""
        import json
        
        output = {
            "source": self.base_url,
            "extracted_at": time.strftime('%Y-%m-%d %H:%M'),
            "page_count": len(self.unique_content),
            "pages": []
        }
        
        for url, data in self.unique_content.items():
            page_data = {
                "url": url,
                "title": data["title"],
                "sections": data.get("sections", []),
                "content_markdown": data["content"],
            }
            output["pages"].append(page_data)
        
        return json.dumps(output, indent=2, ensure_ascii=False)
    
    def to_structured_markdown(self):
        """Export with detailed layout annotations per section"""
        lines = [
            f"# {urlparse(self.base_url).netloc} â€“ Structured Content\n",
            f"*Extracted {time.strftime('%Y-%m-%d %H:%M')}*",
            f"*{len(self.unique_content)} pages*\n",
            "---\n"
        ]
        
        for url, data in self.unique_content.items():
            lines.append(f"## PAGE: {data['title']}\n")
            lines.append(f"**URL:** {url}\n")
            
            if data.get("sections"):
                for i, section in enumerate(data["sections"], 1):
                    lines.append(f"\n### Section {i}: `{section['layout']}`\n")
                    
                    if section.get("heading"):
                        lines.append(f"**Heading:** {section['heading']}")
                    if section.get("subheading"):
                        lines.append(f"**Subheading:** {section['subheading']}")
                    if section.get("has_image"):
                        lines.append(f"**Has Image:** Yes")
                        if section.get("images"):
                            for img in section["images"][:3]:
                                lines.append(f"  - `{img['src'][:80]}...` ({img.get('alt', 'no alt')})")
                    
                    lines.append(f"\n**Content:**\n")
                    # Truncate very long sections
                    content = section.get("content", "")[:2000]
                    if len(section.get("content", "")) > 2000:
                        content += "\n\n*[Content truncated...]*"
                    lines.append(content)
                    lines.append("")
            else:
                lines.append(f"\n{data['content']}\n")
            
            lines.append("\n---\n")
        
        return "\n".join(lines)


# ============ STREAMLIT UI ============
st.set_page_config(
    page_title="Web Scraper",
    page_icon="ðŸ•·ï¸",
    layout="wide"
)

st.title("ðŸ•·ï¸ Web Scraper")
st.markdown("Extract unique text content from any website.")

# Sidebar settings
with st.sidebar:
    st.header("âš™ï¸ Settings")
    
    url = st.text_input(
        "Website URL",
        value="https://example.com",
        help="Enter a full URL. Use 'Single page only' to scrape just this page."
    )
    
    single_page_mode = st.checkbox(
        "ðŸŽ¯ Single page only",
        value=False,
        help="Scrape only the URL entered, don't follow links"
    )
    
    if not single_page_mode:
        max_pages = st.slider(
            "Maximum pages",
            min_value=1,
            max_value=500,
            value=50,
            step=1,
            help="Limit how many pages to crawl"
        )
    else:
        max_pages = 1
    
    delay = st.slider(
        "Request delay (seconds)",
        min_value=0.1,
        max_value=2.0,
        value=0.3,
        step=0.1,
        help="Time between requests (be polite!)"
    )
    
    respect_robots = st.checkbox(
        "Respect robots.txt",
        value=True,
        help="Follow the site's crawling rules"
    )
    
    st.markdown("---")
    st.header("ðŸš« Exclude Paths")
    
    # Initialize session state for the text area widget
    if "exclude_paths_textarea" not in st.session_state:
        st.session_state.exclude_paths_textarea = ""
    
    def add_paths(new_paths):
        """Append new paths to existing, avoiding duplicates"""
        current = st.session_state.exclude_paths_textarea.strip()
        current_set = set(p.strip() for p in current.split("\n") if p.strip())
        new_set = set(p.strip() for p in new_paths.split("\n") if p.strip())
        combined = current_set | new_set  # Union of both sets
        st.session_state.exclude_paths_textarea = "\n".join(sorted(combined))
    
    # Common exclusion presets - BEFORE the text area
    st.markdown("**Quick add:**")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ðŸ›’ E-commerce", use_container_width=True):
            add_paths("/shop/\n/cart/\n/checkout/\n/account/\n/product/\n/products/\n/collection/\n/collections/\n/order/\n/wishlist/")
            st.rerun()
    with col2:
        if st.button("ðŸŒ Languages", use_container_width=True):
            add_paths("/es/\n/fr/\n/de/\n/it/\n/pt/\n/ja/\n/zh/\n/ko/\n/ru/\n/ar/")
            st.rerun()
    
    col3, col4 = st.columns(2)
    with col3:
        if st.button("ðŸ‘¤ User areas", use_container_width=True):
            add_paths("/login/\n/register/\n/account/\n/profile/\n/dashboard/\n/my-account/\n/signin/\n/signup/")
            st.rerun()
    with col4:
        if st.button("ðŸ“° Blog/News", use_container_width=True):
            add_paths("/blog/\n/news/\n/articles/\n/posts/\n/tag/\n/category/\n/author/")
            st.rerun()
    
    # Clear button
    if st.button("ðŸ—‘ï¸ Clear all", use_container_width=True):
        st.session_state.exclude_paths_textarea = ""
        st.rerun()
    
    # Text area - uses key only, no value parameter
    exclude_paths_input = st.text_area(
        "Directories to skip (one per line)",
        height=150,
        help="URLs containing these paths will be skipped. Click buttons above to add common exclusions.",
        key="exclude_paths_textarea"
    )
    
    st.markdown("---")
    st.markdown("**Tips:**")
    st.markdown("""
    - Start with a smaller max pages to test
    - Increase delay if you get blocked
    - Use path exclusions to focus on content
    """)

# Main area
col1, col2 = st.columns([2, 1])

with col1:
    button_label = "ðŸŽ¯ Scrape This Page" if single_page_mode else "ðŸš€ Start Scraping"
    start_button = st.button(button_label, type="primary", use_container_width=True)

# Initialize session state
if "results" not in st.session_state:
    st.session_state.results = None
if "scraper" not in st.session_state:
    st.session_state.scraper = None

if start_button:
    if not url or not url.startswith(("http://", "https://")):
        st.error("Please enter a valid URL starting with http:// or https://")
    else:
        st.session_state.results = None
        
        # Progress UI
        progress_bar = st.progress(0)
        status_text = st.empty()
        stats_container = st.empty()
        
        status_text.text("Initializing...")
        
        try:
            # Parse excluded paths from text area
            exclude_paths = [p.strip() for p in exclude_paths_input.strip().split("\n") if p.strip()]
            
            scraper = WebScraper(
                base_url=url,
                max_pages=max_pages,
                delay=delay,
                respect_robots=respect_robots,
                exclude_paths=exclude_paths,
                single_page_mode=single_page_mode
            )
            
            results = scraper.crawl(progress_bar, status_text, stats_container)
            
            st.session_state.results = results
            st.session_state.scraper = scraper
            
            # Show any status messages
            if scraper.status_messages:
                with st.expander("ðŸ“‹ Crawl Log"):
                    for msg in scraper.status_messages:
                        st.text(msg)
                        
        except Exception as e:
            st.error(f"Error during crawl: {str(e)}")

# Display results
if st.session_state.results:
    st.markdown("---")
    st.header(f"ðŸ“„ Results: {len(st.session_state.results)} pages")
    
    # Export options
    st.subheader("Export")
    export_col1, export_col2, export_col3 = st.columns(3)
    
    with export_col1:
        markdown_content = st.session_state.scraper.to_markdown()
        st.download_button(
            label="ðŸ“ Markdown",
            data=markdown_content,
            file_name=f"scraped_{urlparse(url).netloc}.md",
            mime="text/markdown",
            use_container_width=True
        )
    
    with export_col2:
        structured_md = st.session_state.scraper.to_structured_markdown()
        st.download_button(
            label="ðŸ“ With Layouts",
            data=structured_md,
            file_name=f"scraped_{urlparse(url).netloc}_structured.md",
            mime="text/markdown",
            use_container_width=True
        )
    
    with export_col3:
        json_content = st.session_state.scraper.to_json()
        st.download_button(
            label="ðŸ”§ JSON",
            data=json_content,
            file_name=f"scraped_{urlparse(url).netloc}.json",
            mime="application/json",
            use_container_width=True
        )
    
    # Preview results
    st.markdown("### Preview")
    
    for i, (page_url, data) in enumerate(st.session_state.results.items()):
        # Show layout summary in expander title
        layouts = []
        if data.get("sections"):
            layouts = list(set(s["layout"] for s in data["sections"] if s.get("layout")))
        layout_str = f" [{', '.join(layouts[:3])}]" if layouts else ""
        
        with st.expander(f"**{data['title']}**{layout_str}"):
            st.markdown(f"**URL:** {page_url}")
            
            # Show sections breakdown
            if data.get("sections"):
                st.markdown("**Page Structure:**")
                for j, section in enumerate(data["sections"], 1):
                    layout = section.get("layout", "unknown")
                    heading = section.get("heading", "")[:50] or "(no heading)"
                    has_img = "ðŸ–¼ï¸" if section.get("has_image") else ""
                    st.markdown(f"- Section {j}: `{layout}` â€“ {heading} {has_img}")
            
            st.markdown("---")
            st.markdown("**Content Preview:**")
            st.text(data['content'][:1000] + ("..." if len(data['content']) > 1000 else ""))

# Footer
st.markdown("---")
st.markdown("*Built with Streamlit â€¢ Be respectful when scraping*")
